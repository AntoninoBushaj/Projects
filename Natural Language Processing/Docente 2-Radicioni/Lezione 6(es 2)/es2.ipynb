{"cells":[{"cell_type":"markdown","metadata":{"id":"lurjaaB2_eJb"},"source":["### 1. Consegna\n","\n","Per ogni frame nel FrameSet è necessario assegnare un WN synset ai seguenti elementi:\n","\n","- **Frame name** (nel caso si tratti di una multiword expression, come per esempio 'Religious_belief', disambiguare il termine principale, che in generale è il **sostantivo** se l'espressione è composta da NOUN+ADJ, e il **verbo** se l'espressione è composta da VERB+NOUN; in generale l'elemento fondamentale è individuato come il **reggente dell'espressione**.\n","- **Frame Elements (FEs)** del frame; e \n","- **Lexical Units (LUs)**.\n","\n","I contesti di disambiguazione possono essere creati utilizzando le definizioni disponibili (sia quella del frame, sia quelle dei FEs), ottenendo `Ctx(w)`, il contesto per FN terms `w`.\n","\n","Per quanto riguarda il contesto dei sensi presenti in WN è possibile selezionare glosse ed esempi dei sensi, e dei loro rispettivi iponimi e iperonimi, in modo da avere più informazione, ottenendo quindi il contesto di disambiguazione `Ctx(s)`.\n","\n","\n","\n","### 2. Algoritmi di mapping\n","\n","Il mapping può essere effettuato utilizzando (almeno) uno fra i due approcci descritti nel seguito.\n","\n","- **Approccio a bag of words**, e scelta del senso che permette di massimizzare l'intersezione fra i contesti. In questo caso lo score è calcolato come \n","  $$\n","  score(s,w) = |Ctx(s) \\cap Ctx(w)|+1\n","  $$\n","  sarà selezionato il senso che massimizza lo *score(s,w)*.\n","  \n","- **Approccio grafico**. In questo caso si procede con la costruzione di un grafo che contiene tutti i synset associati ai termini in Framenet (FN)\n","\n","$$\n","  \\text{FN} = w(\\text{FEs}) \\cup w(\\text{LUs})\n","$$\n","### 3. Valutazione dell'output del sistema"]},{"cell_type":"markdown","metadata":{"id":"6EvqV7ZyhVlk"},"source":["#Vari import"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_gA-i72vGOn"},"outputs":[],"source":["import nltk\n","from pprint import pprint\n","nltk.download('framenet_v17')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import framenet as fn\n","from nltk.corpus.reader.framenet import PrettyList\n","from nltk.corpus import wordnet as wn\n","from nltk.stem import WordNetLemmatizer \n","\n","from nltk.corpus import semcor\n","from xml.dom import minidom\n","import random\n","from nltk.corpus.reader.wordnet import Lemma\n","from operator import itemgetter\n","from pprint import pprint\n","\n","import hashlib\n","import random\n","from random import randint\n","from random import seed\n","\n","\"\"\"\n","# Funzione di estrazione dei Frame da analizzare\n","from framenet_extractor import getFrameSetForStudent\n","# Algoritmo di disambiguazione\n","from disambiguation import *\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"IuJy588XvKUm"},"source":["#FrameNet Extractor"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1646661985455,"user":{"displayName":"Aldo Bushaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11068623959070267562"},"user_tz":-60},"id":"_ANkLrxqvJtT"},"outputs":[],"source":["#ritorna gli ID di tutti i frame\n","def get_frams_IDs():\n","    return [f.ID for f in fn.frames()]   \n","#Funzione del Prof per estrarre dei framenet dall'hash del cognome\n","def getFrameSetForStudent(surname, list_len=5):\n","   #n° frame di framenet\n","    nof_frames = len(fn.frames())\n","    base_idx = (abs(int(hashlib.sha512(surname.encode('utf-8')).hexdigest(), 16)) % nof_frames)\n","    print('\\n################################')\n","    print('\\nstudent: ' + surname)\n","    print('\\n################################\\n\\n')\n","    framenet_IDs = get_frams_IDs()\n","    i = 0\n","    offset = 0 \n","    seed(1)\n","    listaFrames = []\n","    while i < list_len:\n","        fID = framenet_IDs[(base_idx+offset)%nof_frames]\n","        f = fn.frame(fID)\n","        listaFrames.append(fID)\n","        fNAME = f.name\n","        offset = randint(0, nof_frames)\n","        print('\\tID: {a:4d}\\tframe: {framename}'.format(a=fID, framename=fNAME))\n","        i += 1         \n","    return listaFrames"]},{"cell_type":"markdown","metadata":{"id":"-Nr7qHFqvoBP"},"source":["#Funzioni ausiliarie"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1646661985457,"user":{"displayName":"Aldo Bushaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11068623959070267562"},"user_tz":-60},"id":"iTZt0bJKvuCG"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer() \n","stop_words_list = []\n","\n","# Ritorna il POS TAG di una parola\n","def get_wordnet_pos(word):\n","    treebank_tag = [tag for (word, tag) in nltk.pos_tag(nltk.word_tokenize(word))][0]\n","    if treebank_tag.startswith('J'):\n","        return wn.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wn.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wn.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wn.ADV\n","    else:\n","        return ''\n","\n","# Ritorna una lista con le stop words\n","def get_stop_words() :\n","    if len(stop_words_list) == 0 :\n","        f = open(\"utils/stop_words_FULL.txt\", \"r\")\n","        for x in f:\n","            stop_words_list.append(x.replace(\"\\n\",\"\"))\n","    return stop_words_list\n","\n","\n","# Calcola il numero di elementi comuni di due liste\n","def num_common_elements_of_lists(list1, list2) :\n","    common_elements = set(list1) & set(list2)\n","    return len(common_elements)\n","\n","\n","# Da una frase ritorna una lista con le singole parole (lemmi) rimuovendo le parole inutili (stop words..)\n","def get_list_of_gains_words(sentence) :\n","    list_words_lemma = []\n","    aus_list_words = sentence.split()\n","    stop_words_list = get_stop_words()\n","\n","    for w in aus_list_words :\n","        word = w.lower().replace(\"'\", \"\")\n","        word = ''.join(e for e in word if e.isalnum())\n","        if word not in stop_words_list :\n","            pos_tag = get_wordnet_pos(w)\n","            if pos_tag != '' :\n","                list_words_lemma.append(lemmatizer.lemmatize(word, pos_tag))\n","\n","    return list_words_lemma\n","\n","\n","# Ritorna la lista delle parole della signature di un synset : gloss + examples\n","def get_signature_of_synset(synset) :\n","    # Gloss\n","    signature = get_list_of_gains_words(synset.definition())\n","    # Examples\n","    for example in synset.examples() :\n","        list_ex = get_list_of_gains_words(example)\n","        signature.extend(list_ex)\n","\n","    return signature"]},{"cell_type":"markdown","metadata":{"id":"V8MWJIJpcLls"},"source":["#Disambiguation"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1646661985459,"user":{"displayName":"Aldo Bushaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11068623959070267562"},"user_tz":-60},"id":"JLTDZI9pcQEW"},"outputs":[],"source":["# Algoritmo Lesk disambiguatore leggermente modificato che aggiunge \n","# alla signature anche quella degli iponimi e degli iperonimi\n","def disambiguation_algorithm(word, context, pos_syn=None) :\n","    best_sense = wn.synsets(word, pos=pos_syn)[0]\n","    max_overlap = 0\n","    for sense in wn.synsets(word, pos=pos_syn) :\n","        # Signature (gloss + examples)\n","        signature_of_sense = get_signature_of_synset(sense)\n","        # Aggiungo le info degli iponomi\n","        for hyponym in sense.hyponyms() :\n","            signature_of_sense.extend(get_signature_of_synset(hyponym))\n","        # Aggiungo le info degli iperonimi\n","        for hypernym in sense.hypernyms() :\n","            signature_of_sense.extend(get_signature_of_synset(hypernym))  \n","              \n","        # Overlap con approccio bag of words (come esplicitato nella formula sommiamo 1)\n","        overlap = num_common_elements_of_lists(context, signature_of_sense) + 1\n","\n","        # aggiorno overlap con il valore più alto in assoluto\n","        if overlap > max_overlap :\n","            max_overlap = overlap\n","            best_sense = sense\n","\n","    return best_sense"]},{"cell_type":"markdown","metadata":{"id":"4Fn8sHY5hQe3"},"source":["#Funzioni utili per la restituzione dei vari contesti"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1646661985462,"user":{"displayName":"Aldo Bushaj","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11068623959070267562"},"user_tz":-60},"id":"cagUybBAB11-"},"outputs":[],"source":["def print_separator():\n","    print('\\n______________________________________________________________________________________________________________________\\n\\n')\n","\n","# Restituisce il contesto del nome di un frame, o del frame element se il flag è specificato come False\n","# Dato dal nome + definizione + frame name in relazione\n","def get_context_of_frame(frame, is_not_frame_element= True):\n","    context = set()\n","\n","    context.add(frame.name.lower()) #Nome\n","    context.update(get_list_of_gains_words(frame.definition)) #Definizione\n","\n","    # se non è un frame element, recupero le sue relazioni\n","    if(is_not_frame_element):\n","      # relazioni di un frame\n","      rels = fn.frame_relations(frame=frame)\n","      for r in rels:\n","          context.add(r.superFrameName.lower())\n","          context.add(r.subFrameName.lower())\n","\n","    return context\n","\n","# Restituisce il contesto di una lexical unit\n","def get_context_for_lu(lu):\n","    context = set()\n","    # faccio lo split sul '.' e prendo la prima parte\n","    # esempio in drench.v, prendo solo drench e lo aggiungo al contesto\n","    context.add((lu.name.split('.')[0]).lower()) \n","    context.update(get_list_of_gains_words(lu.definition)) #Definizione\n","    # aggiungiamo pure i lessemi\n","    for lex in lu.lexemes:\n","        context.add(lex.name.lower())\n","\n","    # fn e cod sono diciture che compaiono nella definizione, le rimuoviamo\n","    if 'fn' in context:\n","        context.remove('fn')\n","    if 'cod' in context:\n","        context.remove('cod')\n","    return context\n","\n","# Da un insieme di parole ritorna la parola reggente\n","def get_reggente_multiwords(words) :\n","    # rimpiazzo underscore con lo spazio\n","    words = words.replace(\"_\", \" \")\n","    #strip() toglie gli spazi bianchi all'inizio e alla fine\n","    words = words.strip()\n","    # Controllo se è una multiwords. \n","    # Es: Verbo + Nome o Nome + Aggettivo\n","    if len(words.split(\" \")) > 1:\n","        #Pos tagging\n","        tokens = nltk.word_tokenize(words)\n","        tags = nltk.pos_tag(tokens)\n","        \n","        # Cerco i verbi, in verbs avrò tutti i verbi \n","        verbs = [(words, tag) for (words, tag) in nltk.pos_tag(nltk.word_tokenize(words)) if tag.startswith('V')]\n","        # Se c'è un verbo è il reggente\n","        if len(verbs) > 0:\n","            # il metodo prende il primo verbo perchè words è sempre un nome di frame,\n","            # per cui non posso avere più verbi insieme ma solo uno con aggettivo o sostantivo\n","            return verbs[0][0]\n","        else: # altrimenti il reggente è il primo dei nomi\n","            nouns = [(words, tag) for (words, tag) in nltk.pos_tag(nltk.word_tokenize(words)) if tag.startswith('N')]\n","            return nouns[0][0]\n","    else:\n","       # se invece è una sola parola non c'è bisogno di fare i controlli di prima\n","        return words\n"]},{"cell_type":"markdown","metadata":{"id":"GKYJRWzXB7NC"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_39qSY_oupYo"},"outputs":[],"source":["############################################################################################\n","\n","if __name__ == '__main__':\n","    cognomi = {\"BushajAntonino\",\"BushajAldo\",\"Frisullo\"}\n","    \n","    for c in cognomi:\n","        list_of_mapped_synset = []\n","        \n","        # Per ogni frame restituito da getFrameSetForStudent\n","        for i in getFrameSetForStudent(c):\n","           # prendo l'i-esimo frame \n","            f = fn.frame(i)\n","            # Stampo il nome del frame\n","            print('\\033[1m' + \"FrameName: \" + '\\033[0m', f.name)\n","        \n","            #Stampo il contesto del frame\n","            context = get_context_of_frame(f)\n","            print('\\033[1m' + \"Ctx: \" + '\\033[0m', context)\n","            \n","            # Determina il synset con algoritmo di disambiguazione e mapping Bag of Words\n","            best_synset = disambiguation_algorithm(get_reggente_multiwords(f.name), context)\n","            # stampo le varie informazioni\n","            print('\\033[1m' + \"BestSense WN Synset: \" + '\\033[0m', best_synset)\n","            print('\\033[1m' + \"Definizione synset: \" + '\\033[0m', best_synset.definition())\n","            list_of_mapped_synset.append(best_synset)\n","            \n","            # Stampo la definizione del frame\n","            print('\\033[1m' + \"\\nDefinizione frame: \" + '\\033[0m', f.definition)\n","            \n","            # In questo ciclo stampo le varie informazioni sui frame elements\n","            print('\\033[1m' + \"\\nFEs: \" + '\\033[0m')\n","            for frame_element_key in f.FE.keys():\n","                # Recupero il frame element corrente\n","                frame_element = f.FE[frame_element_key]\n","                print('\\033[1m' + \"\\tFrame element: \" + '\\033[0m', frame_element.name)\n","                # Prendo il contesto del frame element e lo stampo in output\n","                # il booleano mi indica se sono nel caso in cui sto trattando un frame element o no\n","                context = get_context_of_frame(frame_element,False)\n","                print(\"\\t\", context)\n","                # Stampo in output anche la sua definizione\n","                print('\\033[1m' + \"\\tDefinition: \" + '\\033[0m',frame_element.definition)\n","                try:\n","                    best_synset = disambiguation_algorithm(get_reggente_multiwords(frame_element.name), context)\n","                    print('\\033[1m' + \"\\n\\tBestSense WN Synset: \" + '\\033[0m', best_synset)\n","                    print('\\033[1m' + \"\\tDefinizione synset: \" + '\\033[0m', best_synset.definition())\n","                    list_of_mapped_synset.append(best_synset)\n","                except:\n","                    #wordnet non ha un synset corrispondente a questo frame\n","                    print('\\033[1m' + \"\\n\\tBestSense WN Synset: Nessun risultato \" + '\\033[0m')\n","                    list_of_mapped_synset.append(\"\")\n","                print(\"\\t----------------------------------------------\")\n","\n","            # In questo ciclo stampo le varie informazioni sui lexical units\n","            print('\\033[1m' + \"\\nLUs: \" + '\\033[0m')\n","            for lexical_unit_key in f.lexUnit.keys():\n","                # Recupero il lexical unit corrente\n","                lexical_unit = f.lexUnit[lexical_unit_key]\n","                print('\\033[1m' + \"\\tLexical Unit: \" + '\\033[0m', lexical_unit.name)\n","                # Recupero e stampo in output il contesto\n","                context = get_context_for_lu(lexical_unit)\n","                print(\"\\t\", context)\n","                try:\n","                    best_synset = disambiguation_algorithm(get_reggente_multiwords(str(lexical_unit.name).split(\".\")[0]), context, str(lexical_unit.name).split(\".\")[1])#Nome, contesto, postag\n","                    print('\\033[1m' + \"\\n\\tBestSense WN Synset: \" + '\\033[0m', best_synset)\n","                    print('\\033[1m' + \"\\tDefinizione synset: \" + '\\033[0m', best_synset.definition())\n","                    list_of_mapped_synset.append(best_synset)\n","                except:\n","                    print('\\033[1m' + \"\\n\\tBestSense WN Synset: Nessun risultato \" + '\\033[0m')\n","                    list_of_mapped_synset.append(\"\")\n","                print(\"\\t----------------------------------------------------------------------\")\n","\n","            print_separator()\n","\n","        # Valutazione risultato tra mapping e annotazione\n","        annotation_file = open(\"./annotazione/\" + c + \"_annotazione.txt\", \"r\")\n","        i = 0; mapping_corretti = 0\n","\n","        for line in annotation_file:\n","            # Lettura del synset annotato\n","            annotated_synset = line.split(\";\")[1]\n","            annotated_synset = annotated_synset.strip()\n","            # Confronto tra annotazione e mapping\n","            if str(\"Synset('\"+annotated_synset+\"')\") == str(list_of_mapped_synset[i]) or (str(annotated_synset) == \"\" and str(list_of_mapped_synset[i]) == \"\") :\n","                mapping_corretti+=1\n","            i+=1\n","\n","        print('\\033[1m' + \"Valutazione : \" + '\\033[0m', mapping_corretti/len(list_of_mapped_synset))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"es2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}