{"cells":[{"cell_type":"markdown","source":["#Consegna\n","- **Implementare l’algoritmo di Lesk**\n","1. Estrarre 50 frasi dal corpus SemCor (corpus annotato con i synset di\n","WN) e disambiguare (almeno) un sostantivo per frase. Calcolare\n","l’accuratezza del sistema implementato, sulla base dei sensi annotati in\n","SemCor.\n","2. Randomizzare la selezione delle 50 frasi e la selezione del termine da\n","disambiguare, e restituire l’accuratezza media su (per esempio) 10\n","esecuzioni del programma."],"metadata":{"id":"vmXSQYgGrU5k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YyAfYLsn0dys","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645098203863,"user_tz":-60,"elapsed":353,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYR0hC661WfMZxTQyzWxVGynSV-5PaI6WbLMqR=s64","userId":"00068045890587021093"}},"outputId":"328cb9f9-5b38-4528-a878-2c16d9d792f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package semcor to /root/nltk_data...\n","[nltk_data]   Package semcor is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('semcor')\n","nltk.download('wordnet')\n","from nltk.tree import Tree\n","import statistics\n","\n","from nltk.corpus import wordnet as wn\n","from nltk.corpus import semcor\n","from nltk.stem import WordNetLemmatizer \n","from xml.dom import minidom\n","import random\n","from nltk.corpus.reader.wordnet import Lemma"]},{"cell_type":"markdown","source":["#Funzioni ausiliarie"],"metadata":{"id":"aYQuEjqjtc8_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"r1Wo2EKY-9Pg"},"outputs":[],"source":["lemmatizer = WordNetLemmatizer()\n","stop_words_list = []\n","\n","# Ritorna il POS TAG di una parola\n","def get_wordnet_pos(word):\n","    treebank_tag = [tag for (word, tag) in nltk.pos_tag(nltk.word_tokenize(word))][0]\n","    if treebank_tag.startswith('J'):\n","        return wn.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wn.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wn.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wn.ADV\n","    else:\n","        return ''\n","\n","# Ritorna una lista con le stop words\n","def get_stop_words() :\n","    if len(stop_words_list) == 0 :\n","        f = open(\"utils/stop_words_FULL.txt\", \"r\")\n","        for x in f:\n","            stop_words_list.append(x)\n","\n","    return stop_words_list\n","\n","# Da una frase ritorna una lista con le singole parole (lemmi) rimuovendo le parole inutili (stop words..)\n","def get_list_of_gains_words(sentence) :\n","    list_words_lemma = []\n","    aus_list_words = sentence.split()\n","    stop_words_list = get_stop_words()\n","\n","    for w in aus_list_words :\n","        if w.lower() not in stop_words_list :\n","            pos_tag = get_wordnet_pos(w)\n","            if pos_tag != '' :\n","                list_words_lemma.append(lemmatizer.lemmatize(w.lower(), pos_tag))\n","\n","    return list_words_lemma\n","\n","# Ritorna la lista delle parole della signature di un synset : gloss + examples\n","def get_signature_of_synset(synset) :\n","    # Gloss\n","    signature = get_list_of_gains_words(synset.definition())\n","    # Examples\n","    for example in synset.examples() :\n","        list_ex = get_list_of_gains_words(example)\n","        signature.extend(list_ex)\n","\n","    return signature\n","\n","# Calcola il numero di elementi comuni di due liste\n","def overlap_lists(list1, list2) :\n","    common_elements = set(list1) & set(list2)\n","    return len(common_elements)"]},{"cell_type":"markdown","source":["#Algoritmo Lesk "],"metadata":{"id":"NHwj1Q9btVAK"}},{"cell_type":"code","source":["\n","\"\"\"\n","    Lesk Algorithm che va a disambiguare una parola dato un contesto\n","    :return: il miglior synset disambiguato della parola nel contesto\n","\"\"\"\n","def lesk_algorithm(word, sentence) :\n","    #prendo il primo synset della parola\n","    word_best_sense = wn.synsets(word)[0]\n","    max_overlap = 0\n","\n","    #prendiamo il contesto della parola, ossia la frase senza le parole inutili\n","    context = get_list_of_gains_words(sentence)\n","    #per ogni senso della parola\n","    for current_sense in wn.synsets(word) :\n","        #Prendo la Signature\n","        signature_sense = get_signature_of_synset(current_sense)\n","        # Se la signature è piccola, prendo le info anche dagli iponimi\n","        if len(signature_sense) < 25 : \n","            #Aggiungo le info degli iponomi\n","            for hypo in current_sense.hyponyms() :\n","                signature_sense.extend(get_signature_of_synset(hypo))  \n","              \n","        #Overlap: quante parole in comune tra il contesto e la signature del senso associato,\n","        #è quello che sto cercando\n","        overlap = overlap_lists(context, signature_sense)\n","        #mi interessa il miglior senso con l'overlap maggiore\n","        if overlap > max_overlap :\n","            max_overlap = overlap\n","            word_best_sense = current_sense\n","\n","    return word_best_sense"],"metadata":{"id":"SyC0l4kmtS7n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Lettura frasi da SemCore"],"metadata":{"id":"t6iDVKkWtEbo"}},{"cell_type":"code","source":["\"\"\"\n","   Quello che vogliamo fare è estrarre 50 frasii su un corpus di 100 ma in modo casuale\n","   :return: \n","      -50 frasi casuali prese da Semcor \n","      -parole polisemiche da disambiguare prese dalle 50 frasi\n","\"\"\"\n","def get_semcor_sentence() :\n","    semcor_sentences = []\n","    polysemic_words = []\n","    #Estrae 50 frasi in posizione casuale compresa tra 0 e 100\n","    for index in random.sample(range(0, 100), 50):\n","        elem = []\n","        #tagged_sents() the given file(s) as a list of semcor_sentences. Each sentence is represented as a list of tagged chunks (in tree form).\n","        #tags= both comprende sia il tag pos (part of speech) che il tag sem (semantic)\n","        for noun_tree in semcor.tagged_sents(tag='both')[index]:\n","            #se noun_tree.label è istanza di Lemma, è un nome (pos tag NN) e ha almeno un synset associato\n","            # Il controllo è passato solo da nomi che abbiano almeno un synset\n","            if isinstance(noun_tree.label(), Lemma) and noun_tree[0].label() == 'NN' and len(wn.synsets(noun_tree[0][0])) > 0:\n","                #lo aggiungo alla lista degli elementi\n","                elem.append(noun_tree)\n","        if elem :\n","            #Se elem non è vuota ma contiene le parole polisemiche la aggiungo alla lista \n","            #polysemic_words randomizzando l'ordine con cui vengono messi i nomi\n","            polysemic_words.append(random.choice(elem))\n","            #E aggiungo anche la frase alla lista semcor_sentences\n","            semcor_sentences.append(\" \".join(semcor.sents()[index]))\n","    return semcor_sentences, polysemic_words"],"metadata":{"id":"aKgqeWSltDq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Main"],"metadata":{"id":"Zf12fFnbtOKU"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VsTSWEPluTJC","outputId":"c1561208-679b-455a-d476-bfeb52807389","executionInfo":{"status":"error","timestamp":1645098204472,"user_tz":-60,"elapsed":262,"user":{"displayName":"Antonino Bushaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYR0hC661WfMZxTQyzWxVGynSV-5PaI6WbLMqR=s64","userId":"00068045890587021093"}}},"outputs":[{"output_type":"error","ename":"LookupError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93msemcor\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('semcor')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-b8ab50374256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Lesk Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0msemcor_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolysemic_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_semcor_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mright_sense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-1a430665b6a7>\u001b[0m in \u001b[0;36mget_semcor_sentence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# tagged_sents() the given file(s) as a list of semcor_sentences. Each sentence is represented as a list of tagged chunks (in tree form).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#tags= both comprende sia il tag pos (part of speech) che il tag sem (semantic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnoun_tree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msemcor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m#se noun_tree.label è istanza di Lemma, è un nome (pos tag NN) e ha almeno un synset associato\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Il controllo è passato solo da nomi che abbiano almeno un synset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93msemcor\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('semcor')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"]}],"source":["if __name__ == \"__main__\" :\n","    results=[]\n","    \n","    #Facciamo girare il programma 10 volte facendo una media dei valori ottenuti \n","    for i in range(0,10):\n","        # Lesk Test\n","        semcor_sentences, polysemic_words = get_semcor_sentence()\n","        right_sense = 0\n","    \n","        #invochiamo il Lesk alorith per tutte le 50 frasi estratte da Semcor\n","        for i in range(0,50) :\n","            \n","            #invochiamo il lesk algoritmh passando la prima parola ambigua da disambiguare\n","            #polysemic_words[i])[0][0] significa prendi l'i-esimo albero contenuto in polysemic_words,\n","            # e di quetso albero (label, (pos, nome)) prendi il nome\n","            word_best_sense = lesk_algorithm((polysemic_words[i])[0][0],semcor_sentences[i])\n","\n","            # Se il senso individuato dal lesk algorithm corrisponde a quello\n","            # presente su semcor allora incrementiamo il contatore dei risultati positivi \"right_sense\"\n","            if polysemic_words[i].label() in word_best_sense.lemmas() :\n","                right_sense+=1\n","            \"\"\"\n","            else:\n","                print(\"Frase : \" + semcor_sentences[i])\n","                print(\"Sostantivo : \" + str((polysemic_words[i])[0][0]) + \" -> \" + str((polysemic_words[i])))\n","                print(\"Lesk : \" + str(word_best_sense))\n","                print(\"######################################################\")\n","            \"\"\"\n","\n","        print(\"#CORRECT : {}% \".format(float(right_sense/50)*100) + \"({}/50)\".format(str(right_sense)))\n","        results.append(float(right_sense/50)*100)\n","    \n","    print(\"La media di tutti i punteggi su 10 iterazioni è \" + str(statistics.mean(results)))\n","    \n","    "]},{"cell_type":"code","source":[""],"metadata":{"id":"Uyo4Ojj1PCTF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAdM30jx-9Po"},"source":["#Ouput \n","#CORRECT : 88.0% (44/50)\n","#CORRECT : 90.0% (45/50)\n","#CORRECT : 90.0% (45/50)\n","#CORRECT : 86.0% (43/50)\n","#CORRECT : 86.0% (43/50)\n","#CORRECT : 86.0% (43/50)\n","#CORRECT : 92.0% (46/50)\n","#CORRECT : 94.0% (47/50)\n","#CORRECT : 92.0% (46/50)\n","#CORRECT : 92.0% (46/50)\n","La media di tutti i punteggi su 10 iterazioni è 89.6"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"WSD.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":0}