{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSu76Nz5zViI",
        "outputId": "50e420bd-ed58-482d-8f20-9329e77bf1d0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import semcor\n",
        "from xml.dom import minidom\n",
        "import random\n",
        "from nltk.corpus.reader.wordnet import Lemma\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "stop_words_list = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmMJZQku3oWB"
      },
      "source": [
        "# Ritorna il POS TAG di una parola\n",
        "def get_wordnet_pos(word):\n",
        "    treebank_tag = [tag for (word, tag) in nltk.pos_tag(nltk.word_tokenize(word))][0]\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# Ritorna una lista con le stop words\n",
        "def get_stop_words() :\n",
        "    if len(stop_words_list) == 0 :\n",
        "        f = open(\"stop_words_FULL.txt\", \"r\")\n",
        "        for x in f:\n",
        "            stop_words_list.append(x)\n",
        "    \n",
        "    return stop_words_list\n",
        "\n",
        "# Da una frase ritorna una lista con le singole parole (lemmi) rimuovendo le parole inutili (stop words..)\n",
        "def get_list_of_gains_words(sentence) :\n",
        "    list_words_lemma = []\n",
        "    aus_list_words = nltk.word_tokenize(sentence)\n",
        "    stop_words_list = get_stop_words()\n",
        "\n",
        "    for w in aus_list_words :\n",
        "        if w.lower() not in stop_words_list :\n",
        "            pos_tag = get_wordnet_pos(w)\n",
        "            if pos_tag != '' :\n",
        "                list_words_lemma.append(lemmatizer.lemmatize(w.lower(), pos_tag))\n",
        "\n",
        "    return list_words_lemma\n",
        "\n",
        "# Ritorna la lista delle parole della signature di un synset : gloss + examples\n",
        "def get_signature_of_synset(synset) :\n",
        "    # Gloss\n",
        "    signature = get_list_of_gains_words(synset.definition())\n",
        "    # Examples\n",
        "    for ex in synset.examples() :\n",
        "        list_ex = get_list_of_gains_words(ex)\n",
        "        signature.extend(list_ex)\n",
        "\n",
        "    return signature\n",
        "\n",
        "# Calcola il numero di elementi comuni di due liste\n",
        "def overlap_lists(list1, list2) :\n",
        "    common_elements = set(list1) & set(list2)\n",
        "    return len(common_elements)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1AXoiIWzaJ9"
      },
      "source": [
        "def lesk_algorithm(word, sentence) :\n",
        "    best_sense = wn.synsets(word)[0] #prende il primo synset della parola, ossia il senso più comune\n",
        "    max_overlap = 0\n",
        "    context = get_list_of_gains_words(sentence) # Da una frase ritorna una lista con le singole parole (lemmi) rimuovendo le parole inutili (stop words..)\n",
        "    for sense in wn.synsets(word) :#itero su ogni senso/synset della parola\n",
        "        #Signature\n",
        "        signature_sense = get_signature_of_synset(sense) # Ritorna la lista delle parole della signature di un synset : gloss + examples\n",
        "        if len(signature_sense) < 25 : \n",
        "            #Aggiungo le info degli iponomi ( il contrario di iperonimo es ALBERO (iperonimo) PINO (iponimo))\n",
        "            for hypo in sense.hyponyms() :\n",
        "                signature_sense.extend(get_signature_of_synset(hypo))  \n",
        "              \n",
        "        #Overlap\n",
        "        overlap = overlap_lists(context, signature_sense)\n",
        "\n",
        "        if overlap > max_overlap : #se overlap è maggiore del massimo overlap\n",
        "            max_overlap = overlap #aggiorno l'overlap\n",
        "            best_sense = sense #aggiorno il senso migliore\n",
        "\n",
        "    return best_sense \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_e4AM5ehWUY"
      },
      "source": [
        "def get_filler(subj,obj,semtypes):\n",
        "  #Inserimento filler soggetto e oggetto,  lexname: The name of the lexicographer file containing this synset\n",
        "  if wn.synsets(subj): # verifico che il synset non sia vuoto per evitare IndexError\n",
        "    if wn.synsets(subj)[0].lexname() not in semtypes :\n",
        "        #faccio diventare semtype una lista \n",
        "        lista = []\n",
        "        lista.append(subj.lower())\n",
        "        #semptypes ={\"lexname1\":[subj1], \"lexname2\":[subj2]}\n",
        "        semtypes[wn.synsets(subj)[0].lexname()] = lista # creo la voce nel dizionario semptype con chiave il lexname e contenuto la lista\n",
        "    elif subj.lower() not in semtypes[wn.synsets(subj)[0].lexname()] : \n",
        "        semtypes[wn.synsets(subj)[0].lexname()].append(subj.lower())\n",
        "    #print(wn.synsets(subj)[0].lexname())\n",
        "\n",
        "    if wn.synsets(obj)[0].lexname() not in semtypes :\n",
        "        lista = []\n",
        "        lista.append(obj.lower())\n",
        "        semtypes[wn.synsets(obj)[0].lexname()] = lista\n",
        "    elif obj.lower() not in semtypes[wn.synsets(obj)[0].lexname()] :\n",
        "        semtypes[wn.synsets(obj)[0].lexname()].append(obj.lower())\n",
        "    #print(wn.synsets(obj)[0].lexname())\n",
        "  return semtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67LUT2w2kAGJ"
      },
      "source": [
        "def get_clusters(subj,obj,clusters,cluster_fillers,p):\n",
        "  if wn.synsets(subj): # verifico che il synset non sia vuoto per evitare IndexError\n",
        "    #Aggiungo cluster con i 2 semtypes alla lista generale con le frequenze\n",
        "    clus = \"<\" + wn.synsets(subj)[0].lexname() + \",\" + wn.synsets(obj)[0].lexname()  + \">\"\n",
        "    #Conto le frequenze della coppia di lexname <soggetto,oggetto>, \n",
        "    #alla fine clusters è fatto così:  {<soggetto,oggetto>:contatore, ...}\n",
        "    if clus not in clusters :\n",
        "      clusters[clus] = 1 # se è la prima volta allora il numero è 1\n",
        "    else :\n",
        "      clusters[clus] += 1 # altrimenti incremento di 1 il contatore\n",
        "\n",
        "    cluster_fillers.append(clus)\n",
        "    cluster_fillers.append(p)\n",
        "  return clusters,cluster_fillers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vLyycnUzb_A",
        "outputId": "7e48e571-ba76-4696-c28e-7f4d1b6e25fa"
      },
      "source": [
        "def esercitazione3(verb, file) :\n",
        "    verb_semantics = {}\n",
        "    clusters = {}\n",
        "    cluster_fillers = []\n",
        "\n",
        "    #Lettura file corpus\n",
        "    file1 = open(file,\"r\")\n",
        "    for p in file1 : # per ogni riga nel file input\n",
        "        #try :\n",
        "            #Analisi sintattica con spacy\n",
        "            sent = p\n",
        "            doc=nlp(sent) #costruisce un oggetto documento di spaCy fatto da token https://spacy.io/api/doc\n",
        "            #Scorre i vari token nel parser sintattico della frase\n",
        "            for token in doc:\n",
        "                subj = \"\"\n",
        "                obj = \"\"\n",
        "                # Trova il token che corrisponde al verbo cercato e ne prende subj e obj\n",
        "                if verb in token.text : #se il verbo cercato è nel testo del token\n",
        "                   \n",
        "                    for child in token.children : #per ogni figlio sintattico del token\n",
        "                    #se la relazione sintattica tra il token e il figlio è nsubj https://spacy.io/usage/linguistic-features vedere immagine in fondo\n",
        "                        if child.dep_ == \"nsubj\" and subj == \"\": \n",
        "                            subj = child.text #se il padre è in relazione di soggetto e non l'ho ancora trovato lo aggiungo a subj\n",
        "                        if child.dep_ == \"dobj\" and obj == \"\" :\n",
        "                            obj = child.text   #se il figlio è in relazione di oggetto e non l'ho ancora trovato lo aggiungo a obj             \n",
        "                    #stampo testo token e relazione sintattica del token , poi testo del parent del token e il part-of-speech dall' Universal POS tag set.\n",
        "                    #print(token.text, token.dep_, token.head.text, token.head.pos_)\n",
        "                    \n",
        "                    #Disambiguazione Verbo\n",
        "                   \n",
        "                    verb_synset = lesk_algorithm(token.text, p)\n",
        "                    \n",
        "                    #print(\"----------------Synset verbo : \" + str(verb_synset))\n",
        "                    #print(str(verb_synset.lexname()))\n",
        "\n",
        "                    # Creazione lista semtypes divisa per significato verbo\n",
        "                    if verb_synset not in verb_semantics :\n",
        "                      #crea una voce nel dizionario verb_semantics la cui chiave è verb_synset e il contenuto è un dizionario vuoto\n",
        "                      #quindi verb_semantic è dizionario di dizionari\n",
        "                        verb_semantics[verb_synset] = {} \n",
        "                    \"\"\"\n",
        "                    a={ 1: \"ciao\", 2: \"banana\"}\n",
        "                    b= a[1]\n",
        "                    >>>b = ciao\n",
        "                    verb_semantics è un dizionario, in questo esempio la a, mentre semtypes è il contenuto associato alla chiave \n",
        "                    verb_synset nel dizionario, nel nostro esempio la b\n",
        "                    \"\"\"\n",
        "                    semtypes = verb_semantics[verb_synset] #mettiamo il dizionario associato alla chiave verb_synset in semptypes\n",
        "                    semtypes= get_filler(subj,obj,semtypes)\n",
        "\n",
        "                    clusters,cluster_fillers= get_clusters(subj,obj,clusters,cluster_fillers,p)\n",
        "                    \n",
        "            #print(p)\n",
        "            #print(\"\\n\")\n",
        "        #except IndexError: \n",
        "          #pass # ignora errore index out of range e va avanti\n",
        "    print(\"#Fillers divisi per senso del verbo :\")\n",
        "    print(verb_semantics)\n",
        "    print(\"\\n\")\n",
        "    print(\"#Cluster con frequenze :\")\n",
        "    print(clusters)\n",
        "\n",
        "    print(cluster_fillers)\n",
        "\n",
        "\n",
        "\n",
        "esercitazione3(\"play\", \"corpus Play ridotto.txt\")\n",
        "#esercitazione3(\"show\", \"covid 50000 infect.txt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Fillers divisi per senso del verbo :\n",
            "{Synset('play.n.03'): {'noun.attribute': ['effectiveness'], 'noun.relation': ['part']}, Synset('turn.n.03'): {'noun.substance': ['he'], 'noun.location': ['position'], 'noun.cognition': ['it'], 'noun.act': ['game']}, Synset('play.v.01'): {'noun.substance': ['i', 'he'], 'noun.artifact': ['ball', 'drums'], 'noun.group': ['who'], 'noun.act': ['golf', 'course', 'role'], 'noun.person': ['golfers', 'president', 'sax'], 'noun.communication': ['recitals']}, Synset('play.v.03'): {'noun.group': ['who', 'band'], 'noun.act': ['golf'], 'noun.time': ['march'], 'noun.cognition': ['sorts']}, Synset('play.n.01'): {}, Synset('looseness.n.05'): {'noun.cognition': ['components'], 'noun.relation': ['part']}, Synset('toy.v.02'): {}, Synset('act.v.03'): {'noun.attribute': ['specialty'], 'noun.person': ['man']}}\n",
            "\n",
            "\n",
            "#Cluster con frequenze :\n",
            "{'<noun.attribute,noun.relation>': 1, '<noun.substance,noun.location>': 1, '<noun.group,noun.act>': 2, '<noun.substance,noun.artifact>': 1, '<noun.cognition,noun.relation>': 1, '<noun.cognition,noun.act>': 1, '<noun.person,noun.act>': 2, '<noun.group,noun.time>': 1, '<noun.substance,noun.communication>': 1, '<noun.group,noun.cognition>': 1, '<noun.substance,noun.person>': 1, '<noun.group,noun.artifact>': 1, '<noun.attribute,noun.person>': 1}\n",
            "['<noun.attribute,noun.relation>', 'The effectiveness of the governor in clearing up some of the inconsistencies revolving about the sales tax bill may play a part in determining whether it can muster the required two-thirds vote . \\n', '<noun.substance,noun.location>', \"But ask coach Darrell Royal what position he plays and you'll get the quick response , `` place-kicker '' . \\n\", '<noun.group,noun.act>', 'For a serious young man who plays golf with a serious intensity , Palmer has such an inherent sense of humor that it relieves the strain and keeps his nerves from jangling like banjo strings . \\n', '<noun.substance,noun.artifact>', \"`` I'll do as you say , but I'll also play a provisional ball and get a ruling '' . \\n\", '<noun.cognition,noun.relation>', \"Trouble-free , long-life , quality components will play an increasingly important part in the merchandising of new housing in 1960 '' , Pantas predicted . \\n\", '<noun.cognition,noun.act>', 'it must play a game in which there never is a winner . \\n', '<noun.group,noun.act>', 'The lean and leathery Oklahoma amateur , who has been playing topnotch tournament golf for many years , refused to let the Masters jitters overtake him and closed the tournament with his second straight 69 . \\n', '<noun.person,noun.act>', 'Anyone who now doubted that a personal duel was under way had only to watch how these exceptionally gifted golfers were playing this most difficult golf course . \\n', '<noun.person,noun.act>', 'All week long the President clearly was playing a larger personal role in foreign affairs ; ; \\n', '<noun.group,noun.time>', \"The band should play the Rogues' March as a processional , switching to `` Hail Columbia , Happy Land '' ! ! \\n\", '<noun.substance,noun.communication>', 'Certainly not in Orchestra Hall where he has played countless recitals , and where Thursday night he celebrated his 20th season with the Chicago Symphony Orchestra , playing the Brahms Concerto with his own slashing , demon-ridden cadenza melting into the high , pale , pure and lovely song with which a violinist unlocks the heart of the music , or forever finds it closed . \\n', '<noun.group,noun.cognition>', 'Everywhere there are little touches of humor , and the leader of the on-stage band of musicians is an ebullient comedian who plays all sorts of odd instruments with winning warmth . \\n', '<noun.substance,noun.person>', 'He plays his sax principally for beauty of tone , rather than for scintillating flights of meaningless improvisations , and he has a quiet way of getting back and restating the melody after the improvising is over . \\n', '<noun.group,noun.artifact>', 'Demonstrating the primitive African rhythmic backgrounds of the Blues was Michael Babatunde Olatunji , who plays such native drums as the konga and even does a resounding job slapping his own chest . \\n', '<noun.attribute,noun.person>', 'His normal specialty is playing the good-natured old man , frequently stupid or deluded but never mean or sly . \\n']\n"
          ]
        }
      ]
    }
  ]
}